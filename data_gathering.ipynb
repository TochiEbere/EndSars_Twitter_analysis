{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd01151d0311f71d8e6e4f87d667af00b811bdd0742a1b2917b5461b4f993f97481",
   "display_name": "Python 3.8.5  ('endsars': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1151d0311f71d8e6e4f87d667af00b811bdd0742a1b2917b5461b4f993f97481"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from searchtweets import ResultStream, gen_request_parameters, load_credentials, collect_results"
   ]
  },
  {
   "source": [
    "A hidden yaml file (.yaml) which contains details about my twitter api keys was created."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tweets(start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    This is a function that gets tweets using the premium search API where matching a certain keyword during a\n",
    "    certain date range and returns all these tweets as a list\n",
    "    \"\"\"\n",
    "\n",
    "    #Load the credentials\n",
    "    search_args = load_credentials('~/.twitter_keys.yml',\\\n",
    "    yaml_key='search_tweets_v2')\n",
    "\n",
    "    query=\"\"\"\"#EndSARS\" OR \"End sars\" OR \"endsars\" OR \"Soro Soke\" OR \"lekki toll gate\" OR \"anti-robbery squad\" OR \"lekki massacre\" OR \"End bad governance\" OR \"End swat\" OR #LekkiMassacre lang:en\"\"\"\n",
    "\n",
    "    tweet_fields=\"id,author_id,created_at,text,geo,referenced_tweets,in_reply_to_user_id,entities,public_metrics\" \n",
    "    expansions_fields=\"author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id\" \n",
    "    user_fields=\"id,created_at,location,username,verified,public_metrics\"\n",
    "    place_fields=\"country,country_code,full_name,geo,id,name,place_type\" \n",
    "\n",
    "    query = gen_request_parameters(query, user_fields=user_fields, expansions=expansions_fields, \\\n",
    "        place_fields=place_fields,results_per_call=100, tweet_fields=tweet_fields,\\\n",
    "            end_time=end_date, start_time=start_date\n",
    "            )\n",
    "\n",
    "    tweets = collect_results(query, result_stream_args=search_args, max_tweets=30000)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweets_df(tweet_obj):\n",
    "    \"\"\"\n",
    "    A function that Creates a dataframe from the tweets you've retrieved using the retrieve_tweets function \n",
    "    you created above. This functions should also take a list of datest you want to get tweets from\n",
    "    \"\"\"\n",
    "    tweet_df = pd.DataFrame(tweet_obj)\n",
    "\n",
    "    return tweet_df"
   ]
  },
  {
   "source": [
    "## Fetch data\n",
    "\n",
    "Use functions above to retrieve tweet data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dates_list = [(\"2020-08-01\", \"2020-08-07\"), (\"2020-08-08\", \"2020-08-14\"),\\\n",
    "    (\"2020-08-15\", \"2020-08-21\"), (\"2020-08-22\", \"2020-08-31\"),\\\n",
    "        (\"2020-09-01\", \"2020-09-07\"), (\"2020-09-08\", \"2020-09-14\"),\\\n",
    "            (\"2020-09-15\", \"2020-09-21\"), (\"2020-09-22\", \"2020-09-30\") ]\n",
    "\n",
    "tweet_df = pd.DataFrame()\n",
    "\n",
    "for start, end in dates_list:\n",
    "    tweets = retrieve_tweets(start_date=start, end_date=end)\n",
    "    tweets_ = create_tweets_df(tweets)\n",
    "    tweet_df = pd.concat([tweet_df, tweets_], axis=0, ignore_index=True)\n",
    "\n",
    "# Pickle dat\n",
    "tweet_df.to_pickle('endsars_twitter_data_2.pkl')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}