{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fastText\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#\n",
    "# DATA CLEANING\n",
    "#\n",
    "#####################################################################################\n",
    "\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    if 'ø' in text or  'Ø' in text:\n",
    "        #Do nothing when finding ø \n",
    "        return text   \n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "\n",
    "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    # Standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    #Deal with smileys\n",
    "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    #Strip accents\n",
    "    tweet= strip_accents(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # DO NOT REMOVE STOP WORDS FOR SENTIMENT ANALYSIS - OR AT LEAST NOT NEGATIVE ONES\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + row[4]  \n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(tweet_cleaning_for_sentiment_analysis(row[2].lower())))\n",
    "    return cur_row\n",
    "\n",
    "\n",
    "def preprocess(input_file, output_file, keep=1):\n",
    "    i=0\n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        with open(input_file, 'r', newline='') as csvinfile: #,encoding='latin1'\n",
    "            csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "            for row in csv_reader:\n",
    "                if row[4]!=\"MIXED\" and row[4].upper() in ['POSITIVE','NEGATIVE','NEUTRAL'] and row[2]!='':\n",
    "                    row_output = transform_instance(row)\n",
    "                    csv_writer.writerow(row_output )\n",
    "                i=i+1\n",
    "                if i%10000 ==0:\n",
    "                    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iDAFAdmin\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:357: UserWarning: \"https://t.co/uemdacjg8d\n",
      "\n",
      "#mentalperformance\n",
      "#sportpsychology\n",
      "#tennis\n",
      "#sport\n",
      "#football\n",
      "#soccer\n",
      "#mcfc\n",
      "#fcblive\n",
      "#bulls\n",
      "#tmltalk\n",
      "#arsenal\n",
      "#gohabsgo\n",
      "#weareacmilan\n",
      "#eagles\n",
      "#patriots\n",
      "#sfgiants\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a23ed4c8c22c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'betsentiment-EN-tweets-sentiment-teams.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweets.train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-3e16c5a426a6>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(input_file, output_file, keep)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mcsv_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvinfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"MIXED\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'POSITIVE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NEGATIVE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NEUTRAL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                     \u001b[0mrow_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0mcsv_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_output\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "preprocess('betsentiment-EN-tweets-sentiment-teams.csv', 'tweets.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read10000\n",
      "read20000\n",
      "read30000\n",
      "read40000\n",
      "read50000\n",
      "read60000\n",
      "read70000\n",
      "read80000\n",
      "read90000\n",
      "read100000\n",
      "read110000\n",
      "read120000\n",
      "read130000\n",
      "read140000\n",
      "read150000\n",
      "read160000\n",
      "writer10000\n",
      "writer20000\n",
      "writer30000\n",
      "writer40000\n",
      "writer50000\n",
      "writer60000\n",
      "writer70000\n",
      "writer80000\n",
      "writer90000\n",
      "writer100000\n",
      "writer110000\n",
      "writer120000\n",
      "writer130000\n",
      "writer140000\n",
      "writer150000\n",
      "writer160000\n",
      "writer170000\n",
      "writer180000\n",
      "writer190000\n",
      "writer200000\n",
      "writer210000\n",
      "writer220000\n",
      "writer230000\n",
      "writer240000\n",
      "writer250000\n",
      "writer260000\n",
      "writer270000\n",
      "writer280000\n",
      "writer290000\n",
      "writer300000\n",
      "writer310000\n",
      "writer320000\n",
      "writer330000\n",
      "writer340000\n",
      "writer350000\n",
      "writer360000\n"
     ]
    }
   ],
   "source": [
    "def upsampling(input_file, output_file, ratio_upsampling=1):\n",
    "    # Create a file with equal number of tweets for each label\n",
    "    #    input_file: path to file\n",
    "    #    output_file: path to the output file\n",
    "    #    ratio_upsampling: ratio of each minority classes vs majority one. 1 mean there will be as much of each class than there is for the majority class \n",
    "    \n",
    "    i=0\n",
    "    counts = {}\n",
    "    dict_data_by_label = {}\n",
    "\n",
    "    # GET LABEL LIST AND GET DATA PER LABEL\n",
    "    with open(input_file, 'r', newline='') as csvinfile: \n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "        for row in csv_reader:\n",
    "            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1\n",
    "            if not row[0].split()[0] in dict_data_by_label:\n",
    "                dict_data_by_label[row[0].split()[0]]=[row[0]]\n",
    "            else:\n",
    "                dict_data_by_label[row[0].split()[0]].append(row[0])\n",
    "            i=i+1\n",
    "            if i%10000 ==0:\n",
    "                print(\"read\" + str(i))\n",
    "\n",
    "    # FIND MAJORITY CLASS\n",
    "    majority_class=\"\"\n",
    "    count_majority_class=0\n",
    "    for item in dict_data_by_label:\n",
    "        if len(dict_data_by_label[item])>count_majority_class:\n",
    "            majority_class= item\n",
    "            count_majority_class=len(dict_data_by_label[item])  \n",
    "    \n",
    "    # UPSAMPLE MINORITY CLASS\n",
    "    data_upsampled=[]\n",
    "    for item in dict_data_by_label:\n",
    "        data_upsampled.extend(dict_data_by_label[item])\n",
    "        if item != majority_class:\n",
    "            items_added=0\n",
    "            items_to_add = count_majority_class - len(dict_data_by_label[item])\n",
    "            while items_added<items_to_add:\n",
    "                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])\n",
    "                items_added = items_added + max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))\n",
    "\n",
    "    # WRITE ALL\n",
    "    i=0\n",
    "\n",
    "    with open(output_file, 'w') as txtoutfile:\n",
    "        for row in data_upsampled:\n",
    "            txtoutfile.write(row+ '\\n' )\n",
    "            i=i+1\n",
    "            if i%10000 ==0:\n",
    "                print(\"writer\" + str(i))\n",
    "\n",
    "\n",
    "upsampling( 'tweets.train','uptweets.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "2021-03-20 18:05:12.353434 START=>{'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "Model trained with the hyperparameter \n",
      " {'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "2021-03-20 18:05:55.677515Training complete.{'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "Model is quantized!!\n"
     ]
    }
   ],
   "source": [
    "training_data_path ='uptweets.train' \n",
    "validation_data_path ='tweets.validation'\n",
    "model_path ='\\\\model\\\\'\n",
    "model_name=\"model-en\"\n",
    "\n",
    "def train():\n",
    "    print('Training start')\n",
    "    try:\n",
    "        hyper_params = {\"lr\": 0.01,\n",
    "                        \"epoch\": 20,\n",
    "                        \"wordNgrams\": 2,\n",
    "                        \"dim\": 20}     \n",
    "                               \n",
    "        print(str(datetime.datetime.now()) + ' START=>' + str(hyper_params) )\n",
    "\n",
    "        # Train the model.\n",
    "        model = fasttext.train_supervised(input=training_data_path, **hyper_params)\n",
    "        print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))\n",
    "\n",
    "        # CHECK PERFORMANCE\n",
    "        print(str(datetime.datetime.now()) + 'Training complete.' + str(hyper_params) )\n",
    "        \n",
    "        result = model.test(training_data_path)\n",
    "#         validation = model.test(validation_data_path)\n",
    "        \n",
    "        # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "#         text_line = str(hyper_params) + \",accuracy:\" + str(result[1])  + \",validation:\" + str(validation[1]) + '\\n' \n",
    "#         print(text_line)\n",
    "        \n",
    "        #quantize a model to reduce the memory usage\n",
    "        model.quantize(input=training_data_path, qnorm=True, retrain=True, cutoff=100000)\n",
    "        print(\"Model is quantized!!\")\n",
    "        model.save_model('model.ftz')                \n",
    "    \n",
    "        ##########################################################################\n",
    "        #\n",
    "        #  TESTING PART\n",
    "        #\n",
    "        ##########################################################################            \n",
    "        model.predict(['why not'],k=3)\n",
    "        model.predict(['this player is so bad'],k=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Exception during training: ' + str(e) )\n",
    "\n",
    "\n",
    "# Train your model.\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('endsars')",
   "language": "python",
   "name": "python38564bitendsarsd73fdd1e91f242ed966f5421b7ebc3bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
